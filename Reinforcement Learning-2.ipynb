{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHBAPkXvnwfG",
    "outputId": "b10f2ce3-d934-4f11-fb4d-6f38d3ac83aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent on the grid...\n",
      "Training completed.\n",
      "Starting demonstration...\n",
      "Current position: (1, 1)\n",
      "Selected move: 1\n",
      "Current position: (2, 1)\n",
      "Selected move: 1\n",
      "Current position: (3, 1)\n",
      "Selected move: 1\n",
      "Current position: (4, 1)\n",
      "Selected move: 1\n",
      "Current position: (5, 1)\n",
      "Selected move: 1\n",
      "Current position: (6, 1)\n",
      "Selected move: 1\n",
      "Current position: (7, 1)\n",
      "Selected move: 1\n",
      "Current position: (8, 1)\n",
      "Selected move: 3\n",
      "Current position: (8, 2)\n",
      "Selected move: 3\n",
      "Current position: (8, 3)\n",
      "Selected move: 3\n",
      "Current position: (8, 4)\n",
      "Selected move: 3\n",
      "Current position: (8, 5)\n",
      "Selected move: 3\n",
      "Current position: (8, 6)\n",
      "Selected move: 1\n",
      "Current position: (9, 6)\n",
      "Selected move: 3\n",
      "Current position: (9, 7)\n",
      "Selected move: 3\n",
      "Current position: (9, 8)\n",
      "Selected move: 3\n",
      "Target reached at position: (9, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class GridAgent:\n",
    "    #Define Agent and environemnt\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.value_table = np.zeros((grid_size, grid_size, 4))  # Action-value table\n",
    "        self.explore_prob = 1.0  # Initial exploration probability\n",
    "        self.target = (grid_size - 1, grid_size - 1)  # Target position\n",
    "\n",
    "    #Initilaize the agent\n",
    "    def initialize_position(self):\n",
    "        \"\"\"Initialize the agent at the start of the grid.\"\"\"\n",
    "        self.position = (1,1)\n",
    "        return self.position\n",
    "    #action\n",
    "    def take_action(self, move):\n",
    "        \"\"\"Update position based on the chosen move.\"\"\"\n",
    "        row, col = self.position\n",
    "        if move == 0:  # Move up\n",
    "            row = max(0, row - 1)\n",
    "        elif move == 1:  # Move down\n",
    "            row = min(self.grid_size - 1, row + 1)\n",
    "        elif move == 2:  # Move left\n",
    "            col = max(0, col - 1)\n",
    "        elif move == 3:  # Move right\n",
    "            col = min(self.grid_size - 1, col + 1)\n",
    "\n",
    "        self.position = (row, col)\n",
    "        reward = 1 if self.position == self.target else -1  # Reward\n",
    "        is_done = self.position == self.target\n",
    "        return self.position, reward, is_done\n",
    "\n",
    "    def select_move(self):\n",
    "        \"\"\"Select a move based on exploration or exploitation.\"\"\"\n",
    "        if np.random.rand() < self.explore_prob:\n",
    "            return np.random.randint(4)  # Explore randomly\n",
    "        return np.argmax(self.value_table[self.position])  # Exploit learned values\n",
    "\n",
    "    def learn(self, num_episodes=50000):\n",
    "        \"\"\"Train the agent over a number of episodes.\"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            current_pos = self.initialize_position() #1,0\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                move = self.select_move() # 1\n",
    "                next_pos, reward, finished = self.take_action(move)  #(1,0),-1,False\n",
    "                # Update the value table\n",
    "                best_future_val = np.max(self.value_table[next_pos])\n",
    "                self.value_table[current_pos][move] += 0.1 * (\n",
    "                    reward + 0.9 * best_future_val - self.value_table[current_pos][move]\n",
    "                )\n",
    "                current_pos = next_pos\n",
    "            # Reduce exploration probability gradually\n",
    "            self.explore_prob *= 0.99\n",
    "\n",
    "    def showcase(self):\n",
    "        \"\"\"Showcase the learned behavior of the agent.\"\"\"\n",
    "        current_pos = self.initialize_position()#0,0\n",
    "        reached_target = False\n",
    "        while not reached_target:\n",
    "            print(\"Current position:\", current_pos)\n",
    "            move = np.argmax(self.value_table[current_pos])  # Always exploit\n",
    "            print(\"Selected move:\", move)\n",
    "            current_pos, _, reached_target = self.take_action(move)\n",
    "            time.sleep(2)\n",
    "        print(\"Target reached at position:\", current_pos)\n",
    "\n",
    "\n",
    "# Create and train the agent\n",
    "navigator = GridAgent()\n",
    "\n",
    "print(\"Training the agent on the grid...\")\n",
    "navigator.learn()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "print(\"Starting demonstration...\")\n",
    "navigator.showcase()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
